[
  {
    "path": "posts/2021-01-14-tidyverse/",
    "title": "Introduction to the Tidyverse",
    "description": "An initial training on the philosophy and elements of the *tidyverse*.",
    "author": [
      {
        "name": "Bryan Blanc",
        "url": "https://github.com/bpb824"
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\r\nThis content was presented to Nelson\\Nygaard Staff at a Lunch and Learn webinar on Friday, August 21st, 2020, and is available as a recording here and embedded below\r\n\r\n\r\n\r\n\r\nIntroduction\r\nThe Tidyverse is a ‘universe’ of packages designed for data science with R. They are all built with an underlying design philosophy and grammar based on the concept of ‘tidy’ data. This session will discuss the tidy data concept and its main package implementations. For me, the tidy data concept influences not just the work I do in R but how I structure spreadsheets and datasets in general, and I have found it to generally result in cleaner data analyses.Tidyverse packages are among the most widely used R packages, and greatly improve upon implementations of basic operations in base R.\r\nAcknowledgement: This module heavily draws upon Hadley Wickham’s (the progenitor of the Tidyverse) book (available for free online), R for Data Science, including text, pictures, and code directly copied from that book and slightly modified to suit a shorter narrative. Printed copies are available generally wherever you get your books. It is recommended that you read this book for a deeper understanding of the topics contained hereien – only basic concepts are able to be covered within the time available.\r\nA Note on Packages\r\nFor those that don’t remember what packages are in R – packages are collections of functions developed with a specific purpose in mind; typically a particular topic, data analysis method, or type of data is the focus of a package. Packages include the functions that comprise them as well as documentation, sample datasets for examples, and ‘vignettes’, which are example implementations of the code interspersed with narrative. Depending on how well the package has been documented, there may be more or less of these supplementary items. The most useful packages in R are typically those that are well documented.\r\nThere are thousands of R packages, and because of the open source nature of R, anyone can develop a package and post it online. The two main places to get packages are CRAN (Comprehensive R Archive Network) and GitHub. For packages to be uploaded to CRAN, they must meet a minimum standard of documentation and unit testing of code, to ensure some minimum quality of the packages. Packages on GitHub have no standard – they are hosted by individuals. That said, there is information in the GitHub READMEs about how well the code has been unit tested, and packages are typically only hosted on GitHub in the development phase. If you want the most ‘bleeding edge’ version of a package (maybe some new features in development are really helpful!), go to GitHub, but if you want the most stable version, go to CRAN.\r\nA helpful place to start if you have a goal in mind for your R code but you don’t know which packages can help is the CRAN Task View Page. It is a well maintained list of package topics, including some narrative description of what packages do what within each topic. For example, we might want to check out the Spatial task view. There we can see the many packages developed with spatial analysis in mind.\r\nAnother place to go is Google 🤓 – there is a widespread (and well documented) R community on GitHub, StackOverflow, and various R blogs that can help put you on the right path. Chances are someone has had the same question as you (typically many someones), and if they haven’t, then you might have a package to develop on your hands…\r\nTidyverse Packages\r\nAs mentioned above, the Tidyverse consists of a collection of R packages – several of the key packages are highlighted below.\r\nggplot2, for data visualisation.\r\ndplyr, for data manipulation.\r\ntidyr, for data tidying.\r\nreadr, for data import.\r\npurrr, for functional programming.\r\ntibble, for tibbles, a modern re-imagining of data frames.\r\nstringr, for character strings.\r\nforcats, for factors.\r\nConveniently, Hadley and RStudio created a tidyverse package, which with a single installation command (install.packages(tidyverse)) will install the above packages, and with a single library command (library(tidyverse)) will load those packages into your environment.\r\nYou also get a condensed summary of conflicts with other packages you have loaded:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nYou can see conflicts created later with tidyverse_conflicts():\r\n\r\n\r\nlibrary(MASS)\r\ntidyverse_conflicts()\r\n\r\n\r\n-- Conflicts -------------------------------- tidyverse_conflicts() --\r\nx dplyr::filter() masks stats::filter()\r\nx dplyr::lag()    masks stats::lag()\r\nx MASS::select()  masks dplyr::select()\r\n\r\ndetach(\"package:MASS\", unload=TRUE)\r\n\r\n\r\n\r\nAs well as the core tidyverse, installing this package also installs a selection of other packages that you’re likely to use frequently, but probably not in every analysis. This means they are installed, but not loaded by defaault, so if you want to use them you will need to load them with a library() call. This includes packages for:\r\nWorking with specific types of vectors:\r\nhms, for times.\r\nlubridate, for date/times.\r\n\r\nImporting other types of data:\r\nfeather, for sharing with Python and other languages.\r\nhaven, for SPSS, SAS and Stata files.\r\nhttr, for web apis.\r\njsonlite for JSON.\r\nreadxl, for .xls and .xlsx files.\r\nrvest, for web scraping.\r\nxml2, for XML.\r\n\r\nModeling\r\nmodelr, for modelling within a pipeline\r\nbroom, for turning models into tidy data\r\n\r\nAnd you can check that all tidyverse packages are up-to-date with tidyverse_update():\r\n\r\n\r\ntidyverse_update()\r\n#> The following packages are out of date:\r\n#>  * broom (0.4.0 -> 0.4.1)\r\n#>  * DBI   (0.4.1 -> 0.5)\r\n#>  * Rcpp  (0.12.6 -> 0.12.7)\r\n#> Update now?\r\n#> \r\n#> 1: Yes\r\n#> 2: No\r\n\r\n\r\n\r\nThe Key Parts of the Tidyverse Model of Data Science\r\nHadley proposes a model that nearly all data science projects approximate, described by the below illustration. This course module will mostly focus upon three of these concepts – tidying, transforming, and visualizing. The other topics will be touched on in other course modules. A deeper dive is needed on all these topics for a well rounded data science professional, and it is again recommended to refer to R for Data Science.\r\n\r\n\r\n\r\nTidy Data\r\nWhat is Tidy Data?\r\nYou can represent the same underlying data in multiple ways. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables country, year, population, and cases, but each dataset organises the values in a different way.\r\n\r\n\r\ntable1\r\n\r\n\r\n# A tibble: 6 x 4\r\n  country      year  cases population\r\n  <chr>       <int>  <int>      <int>\r\n1 Afghanistan  1999    745   19987071\r\n2 Afghanistan  2000   2666   20595360\r\n3 Brazil       1999  37737  172006362\r\n4 Brazil       2000  80488  174504898\r\n5 China        1999 212258 1272915272\r\n6 China        2000 213766 1280428583\r\n\r\ntable2\r\n\r\n\r\n# A tibble: 12 x 4\r\n   country      year type            count\r\n   <chr>       <int> <chr>           <int>\r\n 1 Afghanistan  1999 cases             745\r\n 2 Afghanistan  1999 population   19987071\r\n 3 Afghanistan  2000 cases            2666\r\n 4 Afghanistan  2000 population   20595360\r\n 5 Brazil       1999 cases           37737\r\n 6 Brazil       1999 population  172006362\r\n 7 Brazil       2000 cases           80488\r\n 8 Brazil       2000 population  174504898\r\n 9 China        1999 cases          212258\r\n10 China        1999 population 1272915272\r\n11 China        2000 cases          213766\r\n12 China        2000 population 1280428583\r\n\r\ntable3\r\n\r\n\r\n# A tibble: 6 x 3\r\n  country      year rate             \r\n* <chr>       <int> <chr>            \r\n1 Afghanistan  1999 745/19987071     \r\n2 Afghanistan  2000 2666/20595360    \r\n3 Brazil       1999 37737/172006362  \r\n4 Brazil       2000 80488/174504898  \r\n5 China        1999 212258/1272915272\r\n6 China        2000 213766/1280428583\r\n\r\n# Spread across two tibbles\r\ntable4a  # cases\r\n\r\n\r\n# A tibble: 3 x 3\r\n  country     `1999` `2000`\r\n* <chr>        <int>  <int>\r\n1 Afghanistan    745   2666\r\n2 Brazil       37737  80488\r\n3 China       212258 213766\r\n\r\ntable4b  # population\r\n\r\n\r\n# A tibble: 3 x 3\r\n  country         `1999`     `2000`\r\n* <chr>            <int>      <int>\r\n1 Afghanistan   19987071   20595360\r\n2 Brazil       172006362  174504898\r\n3 China       1272915272 1280428583\r\n\r\nThese are all representations of the same underlying data, but they are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse.\r\nThere are three interrelated rules which make a dataset tidy:\r\nEach variable must have its own column.\r\nEach observation must have its own row.\r\nEach value must have its own cell.\r\nThe below illustration shows the rules visually: \"Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells.\r\n\r\n\r\n\r\nThese three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions:\r\nPut each dataset in a tibble.\r\nPut each variable in a column.\r\nIn this example, only table1 is tidy. It’s the only representation where each column is a variable.\r\nWhy ensure that your data is tidy? There are two main advantages:\r\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\r\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. Most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural.\r\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data.\r\nTidying\r\nPivoting\r\nThe principles of tidy data seem so obvious that you might wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons:\r\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\r\nData is often organized to facilitate some use other than analysis. For example, data is often organized to make entry as easy as possible.\r\nThis means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems:\r\nOne variable might be spread across multiple columns.\r\nOne observation might be scattered across multiple rows.\r\nTypically a dataset will only suffer from one of these problems; it’ll only suffer from both if you’re really unlucky! To fix these problems, you’ll need the two most important functions in tidyr: pivot_longer() and pivot_wider().\r\nSeparating and uniting\r\nSo far you’ve learned how to tidy table2 and table4, but not table3. table3 has a different problem: we have one column (rate) that contains two variables (cases and population). To fix this problem, we’ll need the separate() function. You’ll also learn about the complement of separate(): unite(), which you use if a single variable is spread across multiple columns.\r\nSeparate\r\nseparate() pulls apart one column into multiple columns, by splitting wherever a separator character appears. Take table3:\r\n\r\n\r\ntable3\r\n\r\n\r\n# A tibble: 6 x 3\r\n  country      year rate             \r\n* <chr>       <int> <chr>            \r\n1 Afghanistan  1999 745/19987071     \r\n2 Afghanistan  2000 2666/20595360    \r\n3 Brazil       1999 37737/172006362  \r\n4 Brazil       2000 80488/174504898  \r\n5 China        1999 212258/1272915272\r\n6 China        2000 213766/1280428583\r\n\r\nThe rate column contains both cases and population variables, and we need to split it into two variables. separate() takes the name of the column to separate, and the names of the columns to separate into, as shown in Figure 1 and the code below.\r\n\r\n\r\ntable3 %>% \r\n  separate(rate, into = c(\"cases\", \"population\"))\r\n\r\n\r\n# A tibble: 6 x 4\r\n  country      year cases  population\r\n  <chr>       <int> <chr>  <chr>     \r\n1 Afghanistan  1999 745    19987071  \r\n2 Afghanistan  2000 2666   20595360  \r\n3 Brazil       1999 37737  172006362 \r\n4 Brazil       2000 80488  174504898 \r\n5 China        1999 212258 1272915272\r\n6 China        2000 213766 1280428583\r\n\r\n\r\n\r\n\r\nFigure 1: Separating table3 makes it tidy\r\n\r\n\r\n\r\nBy default, separate() will split values wherever it sees a non-alphanumeric character (i.e. a character that isn’t a number or letter). For example, in the code above, separate() split the values of rate at the forward slash characters. If you wish to use a specific character to separate a column, you can pass the character to the sep argument of separate(). For example, we could rewrite the code above as:\r\n\r\n\r\ntable3 %>% \r\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\r\n\r\n\r\n\r\n(Formally, sep is a regular expression, which you’ll learn more about in [strings].)\r\nLook carefully at the column types: you’ll notice that cases and population are character columns. This is the default behaviour in separate(): it leaves the type of the column as is. Here, however, it’s not very useful as those really are numbers. We can ask separate() to try and convert to better types using convert = TRUE:\r\n\r\n\r\ntable3 %>% \r\n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\r\n\r\n\r\n# A tibble: 6 x 4\r\n  country      year  cases population\r\n  <chr>       <int>  <int>      <int>\r\n1 Afghanistan  1999    745   19987071\r\n2 Afghanistan  2000   2666   20595360\r\n3 Brazil       1999  37737  172006362\r\n4 Brazil       2000  80488  174504898\r\n5 China        1999 212258 1272915272\r\n6 China        2000 213766 1280428583\r\n\r\nYou can also pass a vector of integers to sep. separate() will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings; negative value start at -1 on the far-right of the strings. When using integers to separate strings, the length of sep should be one less than the number of names in into.\r\nYou can use this arrangement to separate the last two digits of each year. This make this data less tidy, but is useful in other cases, as you’ll see in a little bit.\r\n\r\n\r\ntable3 %>% \r\n  separate(year, into = c(\"century\", \"year\"), sep = 2)\r\n\r\n\r\n# A tibble: 6 x 4\r\n  country     century year  rate             \r\n  <chr>       <chr>   <chr> <chr>            \r\n1 Afghanistan 19      99    745/19987071     \r\n2 Afghanistan 20      00    2666/20595360    \r\n3 Brazil      19      99    37737/172006362  \r\n4 Brazil      20      00    80488/174504898  \r\n5 China       19      99    212258/1272915272\r\n6 China       20      00    213766/1280428583\r\n\r\nUnite\r\nunite() is the inverse of separate(): it combines multiple columns into a single column. You’ll need it much less frequently than separate(), but it’s still a useful tool to have in your back pocket.\r\n\r\n\r\n\r\nFigure 2: Uniting table5 makes it tidy\r\n\r\n\r\n\r\nWe can use unite() to rejoin the century and year columns that we created in the last example. That data is saved as tidyr::table5. unite() takes a data frame, the name of the new variable to create, and a set of columns to combine, again specified in dplyr::select() style:\r\n\r\n\r\ntable5 %>% \r\n  unite(new, century, year)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  country     new   rate             \r\n  <chr>       <chr> <chr>            \r\n1 Afghanistan 19_99 745/19987071     \r\n2 Afghanistan 20_00 2666/20595360    \r\n3 Brazil      19_99 37737/172006362  \r\n4 Brazil      20_00 80488/174504898  \r\n5 China       19_99 212258/1272915272\r\n6 China       20_00 213766/1280428583\r\n\r\nIn this case we also need to use the sep argument. The default will place an underscore (_) between the values from different columns. Here we don’t want any separator so we use \"\":\r\n\r\n\r\ntable5 %>% \r\n  unite(new, century, year, sep = \"\")\r\n\r\n\r\n# A tibble: 6 x 3\r\n  country     new   rate             \r\n  <chr>       <chr> <chr>            \r\n1 Afghanistan 1999  745/19987071     \r\n2 Afghanistan 2000  2666/20595360    \r\n3 Brazil      1999  37737/172006362  \r\n4 Brazil      2000  80488/174504898  \r\n5 China       1999  212258/1272915272\r\n6 China       2000  213766/1280428583\r\n\r\nCheat Sheet\r\nA handy sheet to have on hand is RStudio’s Data Import Cheat Sheet – this will give you a high level overview of the functions for both reading in data and then tidying. Note that this cheat sheet has not yet been updated with the switch to pivot_* functions from gather() and spread(), but I would expect an update soon.\r\n\r\nTransforming\r\nGeneral\r\nBefore we start an example, a note about transforming, to get us back to the Data Science framework we referred to above. Typically, after you tidy your data, there will be value in additional columns that you add to that data by transforming it in some way - generally these are added using the mutate() function. This can be as simple as extracting the year value from your date column, or something more complex, like assigning a geographic zone to a record (we will do this in the geospatial data module). Transforming is part of your script that adds value after your data has typically already been tidied but before it is used for visuzalation or modeling. We will do some transformations in the below example.\r\nAgain, RStudio has a handy cheat sheet for data transformation, which might be especially useful to you since we did not cover that topic in depth here.\r\n\r\nDates and Times\r\nA key part of the transformations we are doing below has to do with working with date and time data in R, which can have a learning curve – it is recommended to look at this cheat sheet, and for further learning to take a DataCamp course referenced at the bottom of this page. As with anything in R, the best way to learn is by doing.\r\n\r\nExample: Part A\r\nIn submitting your interest for R training, you all generated some data that could use some tidying – let’s take a look at your Doodle responses below.\r\nSome notes on the below code block:\r\nThe Pipe: If you are not familiar with the %>% operator (known as ‘the pipe operator’), meet your new best friend! It comes from the magrittr R package (part of the tidyverse) and gets rid of the need to nest multiple parentheses. Pass the result of one function directly into the first argument of the next.\r\nA note on pivot_* functions: I (Bryan) did not realize prior to developing this course that gather() and spread() were no longer the preferred way of converting between longer and wider data formats, so I am learning this new preferred implementation along with you! Instead of gather(), we should be using pivot_longer(), and instead of spread() we should be using pivot_wider(). The old functions still work but they are no longer being actively improved. If the old functions mean nothing to you, then don’t worry about this note!\r\nThere are many ways to do this: going from messy data to tidy data is not a straight line. You can use the functions below in a different order (or use different functions!) that can take more or less lines of code – do this in the way that best makes sense to you and still gives you the result you want. Experiment using different lines to get to the same result!\r\n\r\n\r\nlibrary(tidyverse) #Loading the tidyverse\r\nlibrary(readxl) #For reading Excel files\r\nlibrary(janitor) #For cleaning up column names\r\nlibrary(forcats) #For implementing ordered categorical \r\nlibrary(hms) #For working with times of day\r\n\r\nraw_doodle = read_excel('data/r_doodle.xls')\r\n\r\n#Take a look at data in R studio viewer and in Excel. \r\n#Column names are weird, the information we want is several rows down from the top. \r\n#Luckily we can use skip some rows and rename columns to make them easier to manipulate\r\n\r\nraw_doodle\r\n\r\n\r\n# A tibble: 26 x 74\r\n   `Poll \"Weekly R~ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 \r\n   <chr>            <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\r\n 1 https://doodle.~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 2 <NA>             <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 3 <NA>             Augu~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 4 <NA>             Mon ~ <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 5 <NA>             8:00~ 8:30~ 9:00~ 11:0~ 11:3~ 12:0~ 12:3~ 1:00~\r\n 6 Mariel Kirschen  OK    <NA>  <NA>  <NA>  <NA>  OK    OK    OK   \r\n 7 Ayaka Habu       <NA>  OK    <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 8 Paul Leitman     OK    OK    OK    OK    OK    OK    <NA>  <NA> \r\n 9 Michael Carraher <NA>  OK    OK    <NA>  <NA>  OK    OK    OK   \r\n10 Esther Needham   OK    OK    OK    <NA>  OK    OK    OK    OK   \r\n# ... with 16 more rows, and 65 more variables: ...10 <chr>,\r\n#   ...11 <chr>, ...12 <chr>, ...13 <chr>, ...14 <chr>, ...15 <chr>,\r\n#   ...16 <chr>, ...17 <chr>, ...18 <chr>, ...19 <chr>, ...20 <chr>,\r\n#   ...21 <chr>, ...22 <chr>, ...23 <chr>, ...24 <chr>, ...25 <chr>,\r\n#   ...26 <chr>, ...27 <chr>, ...28 <chr>, ...29 <chr>, ...30 <chr>,\r\n#   ...31 <chr>, ...32 <chr>, ...33 <chr>, ...34 <chr>, ...35 <chr>,\r\n#   ...36 <chr>, ...37 <chr>, ...38 <chr>, ...39 <chr>, ...40 <chr>,\r\n#   ...41 <chr>, ...42 <chr>, ...43 <chr>, ...44 <chr>, ...45 <chr>,\r\n#   ...46 <chr>, ...47 <chr>, ...48 <chr>, ...49 <chr>, ...50 <chr>,\r\n#   ...51 <chr>, ...52 <chr>, ...53 <chr>, ...54 <chr>, ...55 <chr>,\r\n#   ...56 <chr>, ...57 <chr>, ...58 <chr>, ...59 <chr>, ...60 <chr>,\r\n#   ...61 <chr>, ...62 <chr>, ...63 <chr>, ...64 <chr>, ...65 <chr>,\r\n#   ...66 <chr>, ...67 <chr>, ...68 <chr>, ...69 <chr>, ...70 <chr>,\r\n#   ...71 <chr>, ...72 <chr>, ...73 <chr>, ...74 <chr>\r\n\r\n#Let's try to fix a few things at first\r\nless_raw_doodle = read_excel('data/r_doodle.xls',skip = 3) %>% #Skipping three rows, we will need both weekday and time data from top two new rows\r\n  clean_names() %>%#Cleaning names to make them easier to use, R likes names that don't have spaces, special characters, or leading numbers. \r\n  rename(x2 = august_2020, # We don't need to know that it is August 2020\r\n         name = x1) #Name is going to be our primary identifier \r\n  \r\n#Take a look at the newly read in dataset\r\nless_raw_doodle\r\n\r\n\r\n# A tibble: 23 x 74\r\n   name   x2     x3    x4    x5    x6    x7    x8    x9    x10   x11  \r\n   <chr>  <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\r\n 1 <NA>   Mon 10 <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 2 <NA>   8:00 ~ 8:30~ 9:00~ 11:0~ 11:3~ 12:0~ 12:3~ 1:00~ 1:30~ 2:00~\r\n 3 Marie~ OK     <NA>  <NA>  <NA>  <NA>  OK    OK    OK    OK    OK   \r\n 4 Ayaka~ <NA>   OK    <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \r\n 5 Paul ~ OK     OK    OK    OK    OK    OK    <NA>  <NA>  <NA>  OK   \r\n 6 Micha~ <NA>   OK    OK    <NA>  <NA>  OK    OK    OK    OK    OK   \r\n 7 Esthe~ OK     OK    OK    <NA>  OK    OK    OK    OK    OK    OK   \r\n 8 Ashan~ OK     OK    OK    <NA>  <NA>  <NA>  OK    OK    OK    OK   \r\n 9 Tomok~ <NA>   <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  OK   \r\n10 Techa~ OK     OK    OK    OK    OK    OK    OK    OK    OK    <NA> \r\n# ... with 13 more rows, and 63 more variables: x12 <chr>, x13 <chr>,\r\n#   x14 <chr>, x15 <chr>, x16 <chr>, x17 <chr>, x18 <chr>, x19 <chr>,\r\n#   x20 <chr>, x21 <chr>, x22 <chr>, x23 <chr>, x24 <chr>, x25 <chr>,\r\n#   x26 <chr>, x27 <chr>, x28 <chr>, x29 <chr>, x30 <chr>, x31 <chr>,\r\n#   x32 <chr>, x33 <chr>, x34 <chr>, x35 <chr>, x36 <chr>, x37 <chr>,\r\n#   x38 <chr>, x39 <chr>, x40 <chr>, x41 <chr>, x42 <chr>, x43 <chr>,\r\n#   x44 <chr>, x45 <chr>, x46 <chr>, x47 <chr>, x48 <chr>, x49 <chr>,\r\n#   x50 <chr>, x51 <chr>, x52 <chr>, x53 <chr>, x54 <chr>, x55 <chr>,\r\n#   x56 <chr>, x57 <chr>, x58 <chr>, x59 <chr>, x60 <chr>, x61 <chr>,\r\n#   x62 <chr>, x63 <chr>, x64 <chr>, x65 <chr>, x66 <chr>, x67 <chr>,\r\n#   x68 <chr>, x69 <chr>, x70 <chr>, x71 <chr>, x72 <chr>, x73 <chr>,\r\n#   x74 <chr>\r\n\r\n#We're going to break this data down into a couple of parts and join it back together \r\n\r\n#Time preferences\r\ntime_prefs = less_raw_doodle %>% \r\n  filter(!is.na(name)) %>% #Get only the rows with people's time preferences indicated\r\n  pivot_longer(cols = starts_with('x'), #All columns except name\r\n               names_to = 'key', #Old convention from gather\r\n               values_to='value') #Old convention from gather\r\n\r\n#Time block definitions\r\ntime_blocks = less_raw_doodle %>%\r\n  slice(1:2) %>% #Extract first two rows\r\n  select(-name) %>% #Do not need name column here\r\n  pivot_longer(cols = everything(), #All columns\r\n               names_to = 'key',\r\n               values_to = 'value') %>%\r\n  fill(value,.direction = 'down') %>%\r\n  mutate(col_type = ifelse(str_detect(value,':'),'time','date')) %>%\r\n  pivot_wider(id_cols = key,\r\n              names_from=col_type,\r\n              values_from=value)\r\n\r\n#Join the split dataset back together\r\ntime_prefs_joined = time_prefs %>%\r\n  left_join(time_blocks,by='key') %>%\r\n  select(-key) %>% #Do not need this anymore\r\n  filter(name!='Count') %>%\r\n  mutate(value = case_when(\r\n    value=='OK'~'Yes',\r\n    value=='(OK)'~'Maybe',\r\n    TRUE~'No'\r\n  ),\r\n  value = factor(value,ordered=TRUE,levels = c('Yes','Maybe','No')),\r\n  weekday = str_sub(date,1,3),\r\n  weekday = factor(weekday,ordered=TRUE,levels = c('Mon','Tue','Wed','Thu','Fri'))) %>%\r\n  separate(time,into=c('from_time','to_time'),sep=' – ') %>%\r\n  mutate(from_time = strptime(from_time,format='%I:%M %p') %>%\r\n           as.hms(),\r\n         to_time = strptime(to_time,format='%I:%M %p') %>%\r\n           as.hms()) %>%\r\n  select(-date) %>%\r\n  #this block uses the map() function from purrr, which we will discuss later in the module\r\n  mutate(\r\n    first_name = map_chr(name,function(name){\r\n    (name %>%\r\n      str_split(' ') %>%\r\n      unlist())[1]}),\r\n    last_name = map_chr(name,function(name){\r\n    (name %>%\r\n      str_split(' ') %>%\r\n      unlist())[2]})\r\n  ) %>%\r\n  arrange(desc(last_name),desc(first_name),\r\n          weekday,from_time,to_time) %>%\r\n  mutate(name_factor = factor(name,ordered=TRUE,levels = unique(name)))\r\n\r\n#Look how tidy this data is!!! This will make plotting much easier in next steps\r\ntime_prefs_joined\r\n\r\n\r\n# A tibble: 1,460 x 8\r\n   name  value from_time to_time weekday first_name last_name\r\n   <chr> <ord> <time>    <time>  <ord>   <chr>      <chr>    \r\n 1 Tech~ Yes   08:00     09:00   Mon     Techagumt~ Yanisa   \r\n 2 Tech~ Yes   08:30     09:30   Mon     Techagumt~ Yanisa   \r\n 3 Tech~ Yes   09:00     10:00   Mon     Techagumt~ Yanisa   \r\n 4 Tech~ Yes   11:00     12:00   Mon     Techagumt~ Yanisa   \r\n 5 Tech~ Yes   11:30     12:30   Mon     Techagumt~ Yanisa   \r\n 6 Tech~ Yes   12:00     13:00   Mon     Techagumt~ Yanisa   \r\n 7 Tech~ Yes   12:30     13:30   Mon     Techagumt~ Yanisa   \r\n 8 Tech~ Yes   13:00     14:00   Mon     Techagumt~ Yanisa   \r\n 9 Tech~ Yes   13:30     14:30   Mon     Techagumt~ Yanisa   \r\n10 Tech~ No    14:00     15:00   Mon     Techagumt~ Yanisa   \r\n# ... with 1,450 more rows, and 1 more variable: name_factor <ord>\r\n\r\nVisualizing\r\nThis section will focus on visualizing your data using ggplot2. R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places.\r\nIf you’d like to learn more about the theoretical underpinnings of ggplot2 before you start, I’d recommend reading “The Layered Grammar of Graphics”, http://vita.had.co.nz/papers/layered-grammar.pdf.\r\nFirst steps\r\nLet’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear?\r\nThe mpg data frame\r\nYou can test your answer with the mpg data frame found in ggplot2 (aka ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). mpg contains observations collected by the US Environmental Protection Agency on 38 models of car.\r\n\r\n\r\nmpg\r\n\r\n\r\n# A tibble: 234 x 11\r\n   manufacturer model displ  year   cyl trans drv     cty   hwy fl   \r\n   <chr>        <chr> <dbl> <int> <int> <chr> <chr> <int> <int> <chr>\r\n 1 audi         a4      1.8  1999     4 auto~ f        18    29 p    \r\n 2 audi         a4      1.8  1999     4 manu~ f        21    29 p    \r\n 3 audi         a4      2    2008     4 manu~ f        20    31 p    \r\n 4 audi         a4      2    2008     4 auto~ f        21    30 p    \r\n 5 audi         a4      2.8  1999     6 auto~ f        16    26 p    \r\n 6 audi         a4      2.8  1999     6 manu~ f        18    26 p    \r\n 7 audi         a4      3.1  2008     6 auto~ f        18    27 p    \r\n 8 audi         a4 q~   1.8  1999     4 manu~ 4        18    26 p    \r\n 9 audi         a4 q~   1.8  1999     4 auto~ 4        16    25 p    \r\n10 audi         a4 q~   2    2008     4 manu~ 4        20    28 p    \r\n# ... with 224 more rows, and 1 more variable: class <chr>\r\n\r\nAmong the variables in mpg are:\r\ndispl, a car’s engine size, in litres.\r\nhwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\r\nTo learn more about mpg, open its help page by running ?mpg.\r\nCreating a ggplot\r\nTo plot mpg, run this code to put displ on the x-axis and hwy on the y-axis:\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy))\r\n\r\n\r\n\r\n\r\nThe plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size?\r\nWith ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) creates an empty graph, but it’s not very interesting so I’m not going to show it here.\r\nYou complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter.\r\nEach geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes. ggplot2 looks for the mapped variables in the data argument, in this case, mpg.\r\nA graphing template\r\nLet’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings.\r\n\r\nggplot(data = <DATA>) + \r\n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\r\n\r\nThe rest of this guide will show you how to complete and extend this template to make different types of graphs. We will begin with the <MAPPINGS> component.\r\nAesthetic mappings\r\nIn the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\r\n\r\n\r\n\r\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\r\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue:\r\n\r\n\r\n\r\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car.\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy, color = class))\r\n\r\n\r\n\r\n\r\nTo map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\r\nThe colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines.\r\nIn the above example, we mapped class to the color aesthetic, but we could have mapped class to the size aesthetic in the same way. In this case, the exact size of each point would reveal its class affiliation. We get a warning here, because mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea.\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy, size = class))\r\n\r\n\r\n\r\n\r\nOr we could have mapped class to the alpha aesthetic, which controls the transparency of the points, or to the shape aesthetic, which controls the shape of the points.\r\n\r\n\r\n# Left\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))\r\n\r\n\r\n\r\n# Right\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy, shape = class))\r\n\r\n\r\n\r\n\r\nWhat happened to the SUVs? ggplot2 will only use six shapes at a time. By default, additional groups will go unplotted when you use the shape aesthetic.\r\nFor each aesthetic, you use aes() to associate the name of the aesthetic with a variable to display. The aes() function gathers together each of the aesthetic mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves aesthetics, visual properties that you can map to variables to display information about the data.\r\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values.\r\nYou can also set the aesthetic properties of your geom manually. For example, we can make all of the points in our plot blue:\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\r\n\r\n\r\n\r\n\r\nHere, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an aesthetic manually, set the aesthetic by name as an argument of your geom function; i.e. it goes outside of aes(). You’ll need to pick a level that makes sense for that aesthetic:\r\nThe name of a color as a character string.\r\nThe size of a point in mm.\r\nThe shape of a point as a number, as shown below.\r\n\r\n\r\n\r\nFacets\r\nOne way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\r\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() should be a formula, which you create with ~ followed by a variable name (here “formula” is the name of a data structure in R, not a synonym for “equation”). The variable that you pass to facet_wrap() should be discrete.\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy)) + \r\n  facet_wrap(~ class, nrow = 2)\r\n\r\n\r\n\r\n\r\nTo facet your plot on the combination of two variables, add facet_grid() to your plot call. The first argument of facet_grid() is also a formula. This time the formula should contain two variable names separated by a ~.\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy)) + \r\n  facet_grid(drv ~ cyl)\r\n\r\n\r\n\r\n\r\nIf you prefer to not facet in the rows or columns dimension, use a . instead of a variable name, e.g. + facet_grid(. ~ cyl).\r\nGeometric objects\r\nHow are these two plots similar?\r\n\r\n\r\n\r\nBoth plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In ggplot2 syntax, we say that they use different geoms.\r\nA geom is the geometrical object that a plot uses to represent data. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms, line charts use line geoms, boxplots use boxplot geoms, and so on. Scatterplots break the trend; they use the point geom. As we see above, you can use different geoms to plot the same data. The plot on the left uses the point geom, and the plot on the right uses the smooth geom, a smooth line fitted to the data.\r\nTo change the geom in your plot, change the geom function that you add to ggplot(). For instance, to make the plots above, you can use this code:\r\n\r\n\r\n# left\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy))\r\n\r\n# right\r\nggplot(data = mpg) + \r\n  geom_smooth(mapping = aes(x = displ, y = hwy))\r\n\r\n\r\n\r\nEvery geom function in ggplot2 takes a mapping argument. However, not every aesthetic works with every geom. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the linetype of a line. geom_smooth() will draw a different line, with a different linetype, for each unique value of the variable that you map to linetype.\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))\r\n\r\n\r\n\r\n\r\nHere geom_smooth() separates the cars into three lines based on their drv value, which describes a car’s drivetrain. One line describes all of the points with a 4 value, one line describes all of the points with an f value, and one line describes all of the points with an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive.\r\nIf this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv.\r\n\r\n\r\n\r\nNotice that this plot contains two geoms in the same graph! If this makes you excited, buckle up. We will learn how to place multiple geoms in the same plot very soon.\r\nggplot2 provides over 40 geoms, and extension packages provide even more (see https://exts.ggplot2.tidyverse.org/gallery/ for a sampling). The best way to get a comprehensive overview is the ggplot2 cheatsheet, which you can find at http://rstudio.com/cheatsheets. To learn more about any single geom, use help: ?geom_smooth.\r\nMany geoms, like geom_smooth(), use a single geometric object to display multiple rows of data. For these geoms, you can set the group aesthetic to a categorical variable to draw multiple objects. ggplot2 will draw a separate object for each unique value of the grouping variable. In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.\r\n\r\n\r\nggplot(data = mpg) +\r\n  geom_smooth(mapping = aes(x = displ, y = hwy))\r\n\r\n\r\n\r\nggplot(data = mpg) +\r\n  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))\r\n\r\n\r\n\r\nggplot(data = mpg) +\r\n  geom_smooth(\r\n    mapping = aes(x = displ, y = hwy, color = drv),\r\n    show.legend = FALSE\r\n  )\r\n\r\n\r\n\r\n\r\nTo display multiple geoms in the same plot, add multiple geom functions to ggplot():\r\n\r\n\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy)) +\r\n  geom_smooth(mapping = aes(x = displ, y = hwy))\r\n\r\n\r\n\r\n\r\nThis, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of mappings to ggplot(). ggplot2 will treat these mappings as global mappings that apply to each geom in the graph. In other words, this code will produce the same plot as the previous code:\r\n\r\n\r\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \r\n  geom_point() + \r\n  geom_smooth()\r\n\r\n\r\n\r\nIf you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\r\n\r\n\r\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \r\n  geom_point(mapping = aes(color = class)) + \r\n  geom_smooth()\r\n\r\n\r\n\r\n\r\nYou can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only.\r\n\r\n\r\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \r\n  geom_point(mapping = aes(color = class)) + \r\n  geom_smooth(data = filter(mpg, class == \"subcompact\"), se = FALSE)\r\n\r\n\r\n\r\n\r\nCheat Sheet\r\nAs with the other sections, until you’ve spent a lot of time using the language (and even when you have!) it is hard to remember all the names of functions used and their parameters and syntaxes. Cheat sheets are helpful to have around!\r\n\r\nExample: Part B\r\nBelow, we’re going to take the data we tidied in Part A and make a plot that quickly demonstrates when people are free for R training. Below is a single ggplot code block, with many comments throughout to explain the different pieces. As discussed above, ggplot code is modular – you can add on as many separate pieces as you want (as long as those pieces are not directly conflicting).\r\n\r\n\r\nggplot(time_prefs_joined)+\r\n  geom_rect(aes(\r\n    #Translating times from hms to POSIXct to take advantage of scale_x_datetime()\r\n    #Unfortunately there is not an hms specific scale yet\r\n    xmin=as.POSIXct(from_time),\r\n    xmax=as.POSIXct(to_time),\r\n    #This is a bit of a hack, and there are multiple ways to handle this. \r\n    #Here I am translating the name into a number so I can create a 'width' for the rectangle. \r\n    #Another way to do this would be to use geom_tile()\r\n    ymin = as.numeric(name_factor)-0.5,\r\n    ymax = as.numeric(name_factor)+0.5,\r\n    #The fill color of each rectangle will be determined by what a person filled in the time block in Doodle with. \r\n    fill = value),\r\n    #Making the rectangles semi-transparent since they overlap\r\n    alpha = 0.5)+\r\n  #I want a separate subplot for each weekday\r\n  facet_wrap(~weekday,nrow=1)+\r\n  #Here is the other part of me using the names as ordinal factors to create a rectangle width\r\n  scale_y_continuous(breaks = 1:length(unique(time_prefs_joined$name_factor)),\r\n                     labels = levels(time_prefs_joined$name_factor),\r\n                     limits = c(0.5,(length(unique(time_prefs_joined$name_factor))+0.5)))+\r\n  ggtitle('Doodle Results for Scheduling R Trainings')+\r\n  xlab('Time of Day (PDT)')+ylab('Participant Name')+\r\n  # Here I am using strftime() formatting codes to simplify the time format displayed\r\n  scale_x_datetime(date_labels = '%I %p',date_breaks = '1 hour')+\r\n  #Rotating x axis labels to make them easier to read\r\n  theme(axis.text.x = element_text(angle = 45, hjust=1),\r\n        text = element_text(size=30))\r\n\r\n\r\n\r\n\r\nPurrr\r\nThe purrr package has been around for several years now, but with some recent updates across the tidyverse packages, and my recent introduction via some datacamp courses, it has become a crucial part of my R programming toolbox that I would like to spend some time on.\r\nIteration is a common feature of programming, and one of the main reasons people want to program in the first place – e.g., ‘I want to do this analysis for each sub-unit in my analysis framework’. The typical way to accomplish iteration is through loops, with the most common variety being for loops. For loops are an easy-to-understand way to iterate through an analysis, but they have at least three main drawbacks: 1) they are slow, and 2) a lot of extra unnecessary code is created, 3) they are not modular – pieces of for loops have to be manually copy-pasted, rather than referencing a modular function.\r\npurrr allows for the iterative application of functions to data frames. Additionally, it allows for the nesting of data frames - sometimes we will want to isolate pieces of the iterative analysis, and rather than creating a separate data frame for each iteration, we can nest the iterations into one big data frame.\r\nThe best overview of purrr’s functionality is in the Iteration chapter of R for Data Science. Some of that chapter is copied here, but it is useful to review the entire chapter for a comprehensive understanding.\r\nFor loops vs. functionals\r\nFor loops are not as important in R as they are in other languages because R is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly.\r\nTo see why this is important, consider (again) this simple data frame:\r\n\r\n\r\ndf <- tibble(\r\n  a = rnorm(10),\r\n  b = rnorm(10),\r\n  c = rnorm(10),\r\n  d = rnorm(10)\r\n)\r\n\r\n\r\n\r\nImagine you want to compute the mean of every column. You could do that with a for loop:\r\n\r\n\r\noutput <- vector(\"double\", length(df))\r\nfor (i in seq_along(df)) {\r\n  output[[i]] <- mean(df[[i]])\r\n}\r\noutput\r\n\r\n\r\n[1]  0.3781534 -0.2437868 -0.0372206 -0.2822286\r\n\r\nYou realise that you’re going to want to compute the means of every column pretty frequently, so you extract it out into a function:\r\n\r\n\r\ncol_mean <- function(df) {\r\n  output <- vector(\"double\", length(df))\r\n  for (i in seq_along(df)) {\r\n    output[i] <- mean(df[[i]])\r\n  }\r\n  output\r\n}\r\n\r\n\r\n\r\nBut then you think it’d also be helpful to be able to compute the median, and the standard deviation, so you copy and paste your col_mean() function and replace the mean() with median() and sd():\r\n\r\n\r\ncol_median <- function(df) {\r\n  output <- vector(\"double\", length(df))\r\n  for (i in seq_along(df)) {\r\n    output[i] <- median(df[[i]])\r\n  }\r\n  output\r\n}\r\ncol_sd <- function(df) {\r\n  output <- vector(\"double\", length(df))\r\n  for (i in seq_along(df)) {\r\n    output[i] <- sd(df[[i]])\r\n  }\r\n  output\r\n}\r\n\r\n\r\n\r\nUh oh! You’ve copied-and-pasted this code twice, so it’s time to think about how to generalise it. Notice that most of this code is for-loop boilerplate and it’s hard to see the one thing (mean(), median(), sd()) that is different between the functions.\r\nWhat would you do if you saw a set of functions like this:\r\n\r\n\r\nf1 <- function(x) abs(x - mean(x)) ^ 1\r\nf2 <- function(x) abs(x - mean(x)) ^ 2\r\nf3 <- function(x) abs(x - mean(x)) ^ 3\r\n\r\n\r\n\r\nHopefully, you’d notice that there’s a lot of duplication, and extract it out into an additional argument:\r\n\r\n\r\nf <- function(x, i) abs(x - mean(x)) ^ i\r\n\r\n\r\n\r\nYou’ve reduced the chance of bugs (because you now have 1/3 of the original code), and made it easy to generalise to new situations.\r\nWe can do exactly the same thing with col_mean(), col_median() and col_sd() by adding an argument that supplies the function to apply to each column:\r\n\r\n\r\ncol_summary <- function(df, fun) {\r\n  out <- vector(\"double\", length(df))\r\n  for (i in seq_along(df)) {\r\n    out[i] <- fun(df[[i]])\r\n  }\r\n  out\r\n}\r\ncol_summary(df, median)\r\n\r\n\r\n[1]  0.04642516 -0.10628221 -0.03990710 -0.37053203\r\n\r\ncol_summary(df, mean)\r\n\r\n\r\n[1]  0.3781534 -0.2437868 -0.0372206 -0.2822286\r\n\r\nThe idea of passing a function to another function is an extremely powerful idea, and it’s one of the behaviours that makes R a functional programming language. It might take you a while to wrap your head around the idea, but it’s worth the investment. In the rest of the chapter, you’ll learn about and use the purrr package, which provides functions that eliminate the need for many common for loops. The apply family of functions in base R (apply(), lapply(), tapply(), etc) solve a similar problem, but purrr is more consistent and thus is easier to learn.\r\nThe goal of using purrr functions instead of for loops is to allow you to break common list manipulation challenges into independent pieces:\r\nHow can you solve the problem for a single element of the list? Once you’ve solved that problem, purrr takes care of generalising your solution to every element in the list.\r\nIf you’re solving a complex problem, how can you break it down into bite-sized pieces that allow you to advance one small step towards a solution? With purrr, you get lots of small pieces that you can compose together with the pipe.\r\nThis structure makes it easier to solve new problems. It also makes it easier to understand your solutions to old problems when you re-read your old code.\r\nThe map functions\r\nThe pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output:\r\nmap() makes a list.\r\nmap_lgl() makes a logical vector.\r\nmap_int() makes an integer vector.\r\nmap_dbl() makes a double vector.\r\nmap_chr() makes a character vector.\r\nEach function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function.\r\nOnce you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!).\r\nSome people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years.) The chief benefits of using functions like map() is not speed, but clarity: they make your code easier to write and to read.\r\nWe can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use map_dbl():\r\n\r\n\r\nmap_dbl(df, mean)\r\n\r\n\r\n         a          b          c          d \r\n 0.3781534 -0.2437868 -0.0372206 -0.2822286 \r\n\r\nmap_dbl(df, median)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\nmap_dbl(df, sd)\r\n\r\n\r\n        a         b         c         d \r\n0.9623702 0.7749413 1.0179281 1.1480152 \r\n\r\nCompared to using a for loop, focus is on the operation being performed (i.e. mean(), median(), sd()), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe:\r\n\r\n\r\ndf %>% map_dbl(mean)\r\n\r\n\r\n         a          b          c          d \r\n 0.3781534 -0.2437868 -0.0372206 -0.2822286 \r\n\r\ndf %>% map_dbl(median)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\ndf %>% map_dbl(sd)\r\n\r\n\r\n        a         b         c         d \r\n0.9623702 0.7749413 1.0179281 1.1480152 \r\n\r\nThere are a few differences between map_*() and col_summary():\r\nAll purrr functions are implemented in C. This makes them a little faster at the expense of readability.\r\nThe second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section.\r\nmap_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called:\r\n\r\n\r\nmap_dbl(df, mean, trim = 0.5)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\nThe map functions also preserve names:\r\n\r\n\r\nz <- list(x = 1:3, y = 4:5)\r\nmap_int(z, length)\r\n\r\n\r\nx y \r\n3 2 \r\n\r\nShortcuts\r\nThere are a few shortcuts that you can use with .f in order to save a little typing. Imagine you want to fit a linear model to each group in a dataset. The following toy example splits up the mtcars dataset into three pieces (one for each value of cylinder) and fits the same linear model to each piece:\r\n\r\n\r\nmodels <- mtcars %>% \r\n  split(.$cyl) %>% \r\n  map(function(df) lm(mpg ~ wt, data = df))\r\n\r\n\r\n\r\nThe syntax for creating an anonymous function in R is quite verbose so purrr provides a convenient shortcut: a one-sided formula.\r\n\r\n\r\nmodels <- mtcars %>% \r\n  split(.$cyl) %>% \r\n  map(~lm(mpg ~ wt, data = .))\r\n\r\n\r\n\r\nHere I’ve used . as a pronoun: it refers to the current list element (in the same way that i referred to the current index in the for loop).\r\nWhen you’re looking at many models, you might want to extract a summary statistic like the \\(R^2\\). To do that we need to first run summary() and then extract the component called r.squared. We could do that using the shorthand for anonymous functions:\r\n\r\n\r\nmodels %>% \r\n  map(summary) %>% \r\n  map_dbl(~.$r.squared)\r\n\r\n\r\n        4         6         8 \r\n0.5086326 0.4645102 0.4229655 \r\n\r\nBut extracting named components is a common operation, so purrr provides an even shorter shortcut: you can use a string.\r\n\r\n\r\nmodels %>% \r\n  map(summary) %>% \r\n  map_dbl(\"r.squared\")\r\n\r\n\r\n        4         6         8 \r\n0.5086326 0.4645102 0.4229655 \r\n\r\nYou can also use an integer to select elements by position:\r\n\r\n\r\nx <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\r\nx %>% map_dbl(2)\r\n\r\n\r\n[1] 2 5 8\r\n\r\nThe map functions\r\nThe pattern of looping over a vector, doing something to each element and saving the results is so common that the purrr package provides a family of functions to do it for you. There is one function for each type of output:\r\nmap() makes a list.\r\nmap_lgl() makes a logical vector.\r\nmap_int() makes an integer vector.\r\nmap_dbl() makes a double vector.\r\nmap_chr() makes a character vector.\r\nEach function takes a vector as input, applies a function to each piece, and then returns a new vector that’s the same length (and has the same names) as the input. The type of the vector is determined by the suffix to the map function.\r\nOnce you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!).\r\nSome people will tell you to avoid for loops because they are slow. They’re wrong! (Well at least they’re rather out of date, as for loops haven’t been slow for many years.) The chief benefits of using functions like map() is not speed, but clarity: they make your code easier to write and to read.\r\nWe can use these functions to perform the same computations as the last for loop. Those summary functions returned doubles, so we need to use map_dbl():\r\n\r\n\r\nmap_dbl(df, mean)\r\n\r\n\r\n         a          b          c          d \r\n 0.3781534 -0.2437868 -0.0372206 -0.2822286 \r\n\r\nmap_dbl(df, median)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\nmap_dbl(df, sd)\r\n\r\n\r\n        a         b         c         d \r\n0.9623702 0.7749413 1.0179281 1.1480152 \r\n\r\nCompared to using a for loop, focus is on the operation being performed (i.e. mean(), median(), sd()), not the bookkeeping required to loop over every element and store the output. This is even more apparent if we use the pipe:\r\n\r\n\r\ndf %>% map_dbl(mean)\r\n\r\n\r\n         a          b          c          d \r\n 0.3781534 -0.2437868 -0.0372206 -0.2822286 \r\n\r\ndf %>% map_dbl(median)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\ndf %>% map_dbl(sd)\r\n\r\n\r\n        a         b         c         d \r\n0.9623702 0.7749413 1.0179281 1.1480152 \r\n\r\nThere are a few differences between map_*() and col_summary():\r\nAll purrr functions are implemented in C. This makes them a little faster at the expense of readability.\r\nThe second argument, .f, the function to apply, can be a formula, a character vector, or an integer vector. You’ll learn about those handy shortcuts in the next section.\r\nmap_*() uses … ([dot dot dot]) to pass along additional arguments to .f each time it’s called:\r\n\r\n\r\nmap_dbl(df, mean, trim = 0.5)\r\n\r\n\r\n          a           b           c           d \r\n 0.04642516 -0.10628221 -0.03990710 -0.37053203 \r\n\r\nThe map functions also preserve names:\r\n\r\n\r\nz <- list(x = 1:3, y = 4:5)\r\nmap_int(z, length)\r\n\r\n\r\nx y \r\n3 2 \r\n\r\nMapping over multiple arguments\r\nSo far we’ve mapped along a single input. But often you have multiple related inputs that you need iterate along in parallel. That’s the job of the map2() and pmap() functions. For example, imagine you want to simulate some random normals with different means. You know how to do that with map():\r\n\r\n\r\nmu <- list(5, 10, -3)\r\nmu %>% \r\n  map(rnorm, n = 5) %>% \r\n  str()\r\n\r\n\r\nList of 3\r\n $ : num [1:5] 4.85 5.42 5.22 5.93 2.04\r\n $ : num [1:5] 9 10.16 9.39 8.31 9.31\r\n $ : num [1:5] -1.51 -1.83 -3.79 -1.62 -3.94\r\n\r\nWhat if you also want to vary the standard deviation? One way to do that would be to iterate over the indices and index into vectors of means and sds:\r\n\r\n\r\nsigma <- list(1, 5, 10)\r\nseq_along(mu) %>% \r\n  map(~rnorm(5, mu[[.]], sigma[[.]])) %>% \r\n  str()\r\n\r\n\r\nList of 3\r\n $ : num [1:5] 3.75 5.25 5.45 4.95 5.39\r\n $ : num [1:5] 14.37 2.64 6.8 9.77 4.77\r\n $ : num [1:5] 8.77 0.826 -10.233 -15.813 -3.636\r\n\r\nBut that obfuscates the intent of the code. Instead we could use map2() which iterates over two vectors in parallel:\r\n\r\n\r\nmap2(mu, sigma, rnorm, n = 5) %>% str()\r\n\r\n\r\nList of 3\r\n $ : num [1:5] 6.29 4.89 5.48 6.45 3.75\r\n $ : num [1:5] 21.5 8.29 11.79 3.3 9.36\r\n $ : num [1:5] -2.98 -4.89 -3.88 -20.78 2.6\r\n\r\nmap2() generates this series of function calls:\r\n\r\n\r\n\r\nNote that the arguments that vary for each call come before the function; arguments that are the same for every call come after.\r\nLike map(), map2() is just a wrapper around a for loop:\r\n\r\n\r\nmap2 <- function(x, y, f, ...) {\r\n  out <- vector(\"list\", length(x))\r\n  for (i in seq_along(x)) {\r\n    out[[i]] <- f(x[[i]], y[[i]], ...)\r\n  }\r\n  out\r\n}\r\n\r\n\r\n\r\nYou could also imagine map3(), map4(), map5(), map6() etc, but that would get tedious quickly. Instead, purrr provides pmap() which takes a list of arguments. You might use that if you wanted to vary the mean, standard deviation, and number of samples:\r\n\r\n\r\nn <- list(1, 3, 5)\r\nargs1 <- list(n, mu, sigma)\r\nargs1 %>%\r\n  pmap(rnorm) %>% \r\n  str()\r\n\r\n\r\nList of 3\r\n $ : num 5.07\r\n $ : num [1:3] 3.31 3.05 4.45\r\n $ : num [1:5] -3.37 -3.42 13.67 -4.38 -7.99\r\n\r\nThat looks like:\r\n\r\n\r\n\r\nIf you don’t name the list’s elements, pmap() will use positional matching when calling the function. That’s a little fragile, and makes the code harder to read, so it’s better to name the arguments:\r\n\r\n\r\nargs2 <- list(mean = mu, sd = sigma, n = n)\r\nargs2 %>% \r\n  pmap(rnorm) %>% \r\n  str()\r\n\r\n\r\n\r\nThat generates longer, but safer, calls:\r\n\r\n\r\n\r\nSince the arguments are all the same length, it makes sense to store them in a data frame:\r\n\r\n\r\nparams <- tribble(\r\n  ~mean, ~sd, ~n,\r\n    5,     1,  1,\r\n   10,     5,  3,\r\n   -3,    10,  5\r\n)\r\nparams %>% \r\n  pmap(rnorm)\r\n\r\n\r\n[[1]]\r\n[1] 6.265129\r\n\r\n[[2]]\r\n[1]  7.392727 20.487298 11.494148\r\n\r\n[[3]]\r\n[1]  0.3616743 -4.1692215 -9.8603078 -8.5371038 -0.5070022\r\n\r\nAs soon as your code gets complicated, I think a data frame is a good approach because it ensures that each column has a name and is the same length as all the other columns.\r\nCheat Sheet\r\nAs with the other sections, purrr has a great cheat sheet to help you remember the main uses of the package.\r\n\r\nExample: Part C\r\nWhat if we wanted to clean up the plot we created above up a little bit – we had a bunch of overlapping time blocks because of the way the doodle poll was taken. But what we really care about is when a person is free and when a person is otherwise occupied. Instead of keeping a record of their status for every potential overlapping appointment, can we group together blocks where they are free or occupied for a simpler plot? See how below.\r\n\r\n\r\ntime_prefs_clean = time_prefs_joined %>%\r\n  select(name,name_factor,first_name,last_name,\r\n         weekday,from_time,to_time,value) %>%\r\n  #I want to nest the data specific to each person a data frame to be operated on separately for each iteration\r\n  nest(data = c(weekday,from_time,to_time,value)) %>%\r\n  #I then use map() to apply an anonymous function (a function defined on the fly) below\r\n  mutate(cleaned_time_blocks = map(data,function(data){\r\n    \r\n    #I need to create a group number each time a person's status changes to I can group those time blocks together\r\n    grouped = data %>%\r\n      #Initialize group as 1\r\n      mutate(group_num = 1)\r\n    \r\n    gn = 1\r\n    \r\n    #start on the second entry -- in this case for loops are still necessary \r\n    #because of the tracking of the current group number across rows\r\n    for(i in 2:nrow(grouped)){\r\n      #If the status has changed or the day has changed, create and assign a new group\r\n      if(grouped$value[i] != grouped$value[i-1] |\r\n         grouped$weekday[i] != grouped$weekday[i-1]){\r\n        gn = gn+1\r\n        grouped$group_num[i] = gn\r\n      }else{\r\n        grouped$group_num[i] = gn\r\n      }\r\n    }\r\n    \r\n    #Calculate the bounding times of each group we created\r\n    clean = grouped %>%\r\n      group_by(weekday,group_num,value) %>%\r\n      summarise(min_time = as.hms(min(from_time)),\r\n                max_time = as.hms(max(to_time)))\r\n\r\n    #Return the grouped data frame\r\n    return(clean)\r\n  })) %>%\r\n  #Get rid of the old data frames\r\n  select(-data) %>%\r\n  #Unnest the clean data frames, so we have one big data frame for plotting together\r\n  unnest(cleaned_time_blocks)\r\n\r\n\r\n#Refer to plotting in part B -- very similar implementation\r\nggplot(time_prefs_clean)+\r\n  geom_rect(aes(xmin=as.POSIXct(min_time),\r\n                xmax=as.POSIXct(max_time),\r\n                ymin = as.numeric(name_factor)-0.5,\r\n                ymax = as.numeric(name_factor)+0.5,\r\n                fill = value),\r\n            color='white',size=1)+\r\n  facet_wrap(~weekday,nrow=1)+\r\n  scale_y_continuous(breaks = 1:length(unique(time_prefs_joined$name_factor)),\r\n                     labels = levels(time_prefs_joined$name_factor),\r\n                     limits = c(0.5,(length(unique(time_prefs_joined$name_factor))+0.5)))+\r\n  ggtitle('Cleaner Doodle Results for Scheduling R Trainings')+\r\n  xlab('Time of Day (PDT)')+ylab('Participant Name')+\r\n  scale_x_datetime(date_labels = '%I %p',date_breaks = '1 hour')+\r\n  theme(axis.text.x = element_text(angle = 45, hjust=1),\r\n        text = element_text(size=30))\r\n\r\n\r\n\r\n\r\nViola! That is it for this lesson – we have spent some time on tidying, transforming, and visualizing. Of course your learning does not end here – I have included a number of resources for review below that will help you delve deeper into and master these topics.\r\nReference Materials\r\nRun Through the Example Code Yourself\r\nYou can run through the example yourself based on the example code and data, which you can get by cloning (or just downloading as a ZIP) the GitHub repository for this course and navigating to the topics_setup/01_tidyverse directory. There you will see example_code.R, and the raw Doodle results in the data folder.\r\nCheat Sheets\r\nData import\r\nData Transformation with dplyr\r\nData Visualization with ggplot2\r\nDates and times with lubridate\r\nApply functions with purrr\r\nFurther Reading\r\nR for Data Science, by Garrett Grolemund and Hadley Wickham. As mentioned above, some of the content for this module is copied directly from that book.\r\nRelated DataCamp Courses\r\nTidyverse Fundamentals course track (5 courses, 20 hours)\r\nIntermediate Tidyverse Toolbox course track (4 courses, 16 hours)\r\nWorking with Dates and Times in R (1 course, 4 hours)\r\nData Visualization with R course track (3 courses, 12 hours)\r\nThis content was presented to Nelson\\Nygaard Staff at a Lunch and Learn webinar on Friday, August 21st, 2020, and is available as a recording here and embedded at the top of the page.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-14T08:56:56-08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Nelson\\Nygaard R Training",
    "description": "Welcome to Nelson\\Nygaard's new R Training Blog!",
    "author": [
      {
        "name": "Bryan Blanc",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-14T08:48:39-08:00",
    "input_file": {}
  }
]
